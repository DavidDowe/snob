%% This document is part of the snob project
%% Copyright 2017 the authors. All rights reserved.
\documentclass{elsarticle}
\journal{Journal of Artificial Intelligence}
\usepackage{bm,url}
%\usepackage{boldsymbol}
\usepackage{verbatim} % for \begin{comment}'ing out sections.

% For revision history
\IfFileExists{vc.tex}{\input{vc.tex}}{
	\newcommand{\githash}{UNKNOWN}
	\newcommand{\giturl}{UNKNOWN}}

% Define commands
\newcommand{\article}{\emph{Article}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\project}[1]{\textsl{#1}}

% Surveys
\newcommand{\apogee}{\acronym{APOGEE}}
\newcommand{\ges}{\acronym{GES}}
\newcommand{\hermes}{\acronym{HERMES}}
\newcommand{\galah}{\acronym{GALAH}}
\newcommand{\fourmost}{\acronym{4MOST}}
\newcommand{\weave}{\acronym{WEAVE}}
\newcommand{\gaia}{\project{Gaia}}

% Common terms
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}

\def\teff{T_{\rm eff}}
\def\logg{\log{g}}

% Have some maaaath.
\def\infinity{\rotatebox{90}{8}}
\def\veccov{\vect{C}}
\def\vecmean{\vect{\mu}}
\def\vectheta{\vect{\theta}}
\def\weight{w}
\def\weights{\vect{\weight}}
\def\datum{y}
\def\data{\vect{\datum}}
\def\likelihood{\mathcal{L}}

% Affiliation(s)
\newcommand{\moca}{
	\affil{School of Physics and Astronomy, Monash University, 
		Melbourne, Clayton VIC 3800, Australia}}
\newcommand{\claytonfit}{
	\affil{Faculty of Information Technology, Monash University,
		Melbourne, Clayton VIC 3800, Australia}}
\newcommand{\caulfieldfit}{
	\affil{Faculty of Information Technology, Monash University,
		Melbourne, Caulfield East VIC 3145, Australia}}



\begin{document}
	\begin{frontmatter}
		\title{A search method for Gaussian mixtures using MML}
		
		%\author{Aleti, Casey, Dowe, Lattanzio}
		
		\begin{abstract}
			
		\end{abstract}
		
		
		\begin{keyword}
			Search\sep mml
		\end{keyword}
	\end{frontmatter}
\section{Introduction} 
\label{sec:introduction}


There are $N$ data points each with $D$ dimensions, which are to be modelled
by $K$ mixture of $D$-dimensional gaussian distributions, each with a relative
weighting $\weight_k$ such that $\sum_{k=1}^{K}\weight_k = 1$. 
The data have the same error value, $y_{err}$, in all $D$ dimensions, for all $N$ observations.

The full expression for the message length of a
given by,

\begin{eqnarray}
I_K & = & K\log{2} % I_k
    + \frac{(K - 1)}{2}\log{N} - \frac{1}{2}\sum_{k=1}^{K}\log{w_k} - \log{|\Gamma(K)|} % I_w
    + \mathcal{L}(\data|\vectheta) - DN\log{y_{err}} \\ % Likelihood \\
  & + & \frac{1}{2}\sum_{k=1}^{K}\left[\frac{D(D+3)}{2}\log{{Nw_k}} - (D + 2)\log{|\veccov_k|} - D\log{2}\right] % I_t
    - \frac{Q}{2}\log(2\pi) + \frac{\log{Q\pi}}{2} % lattice, minus the lattice of part 2
\end{eqnarray}

\noindent{}where $Q = \frac{1}{2}DK(D + 3) + K - 1$, the number of free parameters, and
$\mathcal{L}$ is the log-likelihood of a multivariate gaussian distribution.

Say we wanted to calculate whether another mixture was warranted. If another
mixture were preferred then we would want:

\begin{eqnarray}
  \Delta{}I_{K+1} - I_{K} < 0
\end{eqnarray}

The expression is given as:
\begin{eqnarray}
\Delta{I_{K+1} - I_K} & = & (K + 1)\log{2} - K\log{2} \\ % I_k^(new) - %I_k^(old)
  &&+ \frac{(K)}{2}\log{N} - \frac{1}{2}\sum_{k=1}^{K+1}\log{w_k}^{(new)} - \log{|\Gamma(K+1)|} \\ % I_w^(new)
  &&- \frac{(K - 1)}{2}\log{N} + \frac{1}{2}\sum_{k=1}^{K}\log{w_k} + \log{|\Gamma(K)|}\\ % I_w^(old)
  &&+ \mathcal{L}^{(new)} - DN\log{y_{err}} \\ % Likelihood (new)\\
  &&- \mathcal{L}^{(old)} + DN\log{y_{err}} \\ % Likelihood (old) \\
  &&+ \frac{1}{2}\sum_{k=1}^{K+1(new)}\left[\frac{D(D+3)}{2}\log{{Nw_k}} - (D + 2)\log{|\veccov_k|} - D\log{2}\right] \\ % I_t^(new)
  &&- \frac{1}{2}\sum_{k=1}^{K(old)}\left[\frac{D(D+3)}{2}\log{{Nw_k}} - (D + 2)\log{|\veccov_k|} - D\log{2}\right] \\ % I_t^(old)
  &&- \frac{Q^{(new)}}{2}\log(2\pi) + \frac{\log{Q^{(new)}\pi}}{2} \\ % lattice, minus the lattice of part 2
  &&+ \frac{Q^{(old)}}{2}\log(2\pi) - \frac{\log{Q^{(old)}\pi}}{2} % lattice, minus the lattice of part 2
\end{eqnarray}

\noindent{}By making use of $\log{\Gamma(K)} - \log{\Gamma(K + 1)} = -\log{K}$ and re-arranging the expression:

\begin{eqnarray}
\Delta{}I_{K+1} - I_K &=& \log{2} % \Delta{}I_k
    + \frac{1}{2}\log{N} - \log{K} - \frac{1}{2}\left(\sum_{k=1}^{K+1}\log{w_k^{(new)}} - \sum_{k=1}^{K}\log{w_k^{(old)}}\right) \nonumber \\ % \Delta{}I_w
&& + \mathcal{L}^{(new)} - \mathcal{L}^{(old)} \nonumber \\ % likelihood
&& + \frac{1}{2}\left[\frac{D(D+3)}{2}\left(\sum_{k=1}^{K+1}\log{Nw_k^{(new)} - \sum_{k=1}^{K+1}\log{Nw_k^{(old)}}} \right) - \left(D+2\right)\left(\sum_{k=1}^{K+1}\log{|\veccov_k|^{(new)}} - \sum_{k=1}^{K+1}\log{|\veccov_k|^{(old)}}\right)\right] \nonumber \\
&& + \frac{\log(2\pi)}{2}(Q^{(old)} - Q^{(new)}) + \frac{\pi}{2}\left(\log{Q^{(new)}} - \log{Q^{(old)}}\right)
\label{eq:13}
\end{eqnarray}

Expanding the $Q$ terms:

\begin{eqnarray}
Q^{(old)} - Q^{(new)} &=& \frac{1}{2}DK(D + 3) + K - 1 - \frac{1}{2}D(K + 1)(D + 3) + (K + 1) - 1 \nonumber \\
Q^{(old)} - Q^{(new)} &=& -\frac{1}{2}D(D+3) + 2K  - 1
\label{eq:14}
\end{eqnarray}

\noindent{}And making use of the following logarithmic identities,
\begin{eqnarray}
  \log{Q^{(new)}} &=& \log{\left(\frac{1}{2}D(K+1)(D + 3) + K\right)} \nonumber \\
                  &=& \log{\left(\frac{1}{2}D(K+1)(D + 3)\right)} + \log{\left(1 + \frac{K}{\frac{1}{2}D(K+1)(D + 3)}\right)} \\
  \log{Q^{(old)}} &=& \log{\left(\frac{1}{2}DK(D + 3) + K - 1\right)} \nonumber \\
                  &=& \log{\left(\frac{1}{2}DK(D + 3)\right)} + \log{\left(1 + \frac{K - 1}{\frac{1}{2}DK(D + 3)}\right)}
\end{eqnarray}


\noindent{}gives us,

\begin{eqnarray}
  \log{Q^{(new)}} - \log{Q^{(old)}} &=& \log{\left(\frac{1}{2}D(K+1)(D + 3)\right)} - \log{\left(\frac{1}{2}DK(D + 3)\right)} \nonumber \\
                                    &&+ \log{\left(1 + \frac{K}{\frac{1}{2}D(K+1)(D + 3)}\right)} - \log{\left(1 + \frac{K - 1}{\frac{1}{2}DK(D + 3)}\right)} \quad .\\
\end{eqnarray}
The second row of terms can be ignored because they are very small (typically less than 1 bit). This is because as $K \rightarrow \infinity$, $2K/D(K+1)(D+3) \rightarrow 1$, thus $\log{\left(1 + \frac{K}{\frac{1}{2}D(K+1)(D + 3)}\right)} \rightarrow \log{2}$. Similarly as $D \rightarrow \infinity$, $2K/D(K+1)(D+3) \rightarrow 0$.

As $K \rightarrow \infinity$ then $2(K-1)/DK(D+3) \rightarrow 1$ and as $D \rightarrow \infinity$ then $2(K-1)/DK(D+3) \rightarrow 0$ and thus $\log{\left(1 + \frac{K - 1}{\frac{1}{2}DK(D + 3)}\right)} \approx 0$.

\noindent{}Ignoring these minor terms:

\begin{eqnarray}
  \log{Q^{(new)}} - \log{Q^{(old)}} &\approx& \log{\left(\frac{1}{2}D(K+1)(D + 3)\right)} - \log{\left(\frac{1}{2}DK(D + 3)\right)} \nonumber \\
  \log{Q^{(new)}} - \log{Q^{(old)}} &\approx& \log{(K + 1)} - \log{K}
  \label{eq:19}
\end{eqnarray}

\noindent{}Substituting Eqs. \ref{eq:19} and \ref{eq:14} into \ref{eq:13} yields:

\begin{eqnarray}
\Delta{}I_{K+1} - I_K &\approx& \log{2} % \Delta{}I_k
    + \frac{1}{2}\log{N} - \log{K} - \frac{1}{2}\left(\sum_{k=1}^{K+1}\log{w_k^{(new)}} - \sum_{k=1}^{K}\log{w_k^{(old)}}\right) \nonumber \\ % \Delta{}I_w
&& + \mathcal{L}^{(new)} - \mathcal{L}^{(old)} \nonumber \\ % likelihood
&& + \frac{1}{2}\left[\frac{D(D+3)}{2}\left(\sum_{k=1}^{K+1}\log{Nw_k^{(new)} - \sum_{k=1}^{K+1}\log{Nw_k^{(old)}}} \right) - \left(D+2\right)\left(\sum_{k=1}^{K+1}\log{|\veccov_k|^{(new)}} - \sum_{k=1}^{K+1}\log{|\veccov_k|^{(old)}}\right)\right] \nonumber \\
&& + \frac{\log(2\pi)}{2}(-\frac{1}{2}D(D+3) + 2K  - 1) + \frac{\pi}{2}\left(\log{(K + 1)} - \log{K}\right)
\end{eqnarray}

\section{The k-means++ algorithm}

The k-means++ algorithm extends the k-means method with an efficient way of choosing centres. Lets $D(x)$ denote the shortest distance from a data point $x$ to the closest centre that has already been selected. The k-means++ performs the following steps:

\begin{enumerate}
	\item Selects a centre $c_1$ uniformly at random from the $N$ data points.
	\item Selects a new centre $c_i$ from the data points with probability $\frac{D(x)^2}{\sum_{x\in X}}D(x)^2$
	\item Repeat Step 2. until k centres have been selected.
	\item Proceed as with the standard k-means algorithm.
\end{enumerate}

k-means++ is O(log k)-Competitive.

\section{Email from David}

Re the case of two (or more) classes with the same mean:
In 2D, a simple case in point is two classes with the same mean
such that $sigma_{1, x} = sigma_{2, y} >> sigma_{2, x} = sigma_{1, y}$.
Andy said that this and higher dimensional stuff can be picked up
by dropping to 1 dimension along the eigenvector of largest eigenvalue.
I like that.
I wonder also about the merits of dropping to 2 dimensions (if we have
at least 2 attributes), being the eigenvectors of the two largest eigenvalues.
If it could be done quickly, one could transform to a circle, and then
some test (e.g., Kolmogorov-Smirnov test) for whether data is uniform
in radial distribution [0, 2 $\pi$).  If not uniform, then perhaps grounds for
splitting.
One could possibly check whether uniform radially directly from the
original 2D projection without having to project it to a circle (i.e., it
might be pretty easy to work out what the angle of each point would
be post-transformation without having to do the transformation).  The
2D projection is 1D in the sense that we only care about the angle
around the circle.  And, again, Kolmogorov-Smirnov is a popular test
for uniformity.
Re looking at things in 1 dimension (not 2D followed by 1D [0, 2 pi) angle
as above but rather projecting on eigenvector of largest eigenvalue), we
discussed moments - e.g., \url{https://en.wikipedia.org/wiki/Normal_distribution} .
So, 2nd, 4th, 6th and 8th moments of a true Gaussian are 1, 3, 15, 105.
The heuristic I'm going to propose is independent (i.e., I haven't heard
of it before), but it seems pretty easy, it mightn't be original, and I can
conceive that Andy might already be using it.
Let's suppose we have a 1D component.  We'll shift it (without loss of
generality, w.l.o.g.) so that its mean is 0 and we'll shrink or expand it
(again, w.l.o.g.) so that its s.d. ($\sigma$) is 1 (and variance v is 1).
We consider now splitting it into K components with mean 0,
mixing weights (or proportions) $p_1, ..., p_i, ..., p_K$ and s.d.s $sigma_i$ (and variances $v_i = (sigma_i)^2$).
Let's set $K = 2, so p_2 = 1 - p_1$.
Suppose the moments of our current component are 1, 3 $alpha_4, 15 alpha_6 and 105 alpha_8, ... $.
We get some equations - if we have the correct number of equations,
they're simultaneous equations.
If we get too many equations (by using too many moments), then it
becomes something of a fitting or regression problem to choose
$p_1, ..., p_i, ..., p_K$ and s.d.s $sigma_i$, variances $v_i = (sigma_i)^2$.
Our simultaneous equations are:
$p_1 v_1       + (1 - p_1) v_2       =    1$
$p_1 (v_1)^2  + (1 - p_1) (v_2)^2  =    3 alpha_4$
$p_1 (v_1)^3  + (1 - p_1) (v_2)^3  =   15 alpha_6$
$p_1 (v_1)^4  + (1 - p_1) (v_2)^4  =  105 alpha_8$
(and possibly etc., as possibly needed).

These might be messy to solve on the fly    but
one could pre-process by solving many of these beforehand
(actually, approximately solving these beforehand, as it's only
a heuristic, anyway) at the very start of the program while
loading in the data.
(I remember that Chris Wallace got me to do this a similar
pre-processing step when dealing with von Mises angular/circular data.)
So, for $1000 x 1000 = 10^6$ different values of $alpha_4 and alpha_6$
or for $100 x 100 x 100 = 10^6$ different values of $alpha_4, alpha_6 and alpha_8$
one could pre-compute {\em approximate} (rough heuristic)
1,000,000ish or so `solutions' to the above simultaneous equations.
Then, when running the program with real actual values of $alpha_4$ and $alpha_6$ (and $alpha_8$),
one could just grab something from nearby in this pre-computed look-up table
as putative values for $p_1, v_1, v_2$ (and etc., if need be).

I'm not sure whether you're already doing (something like) this.
If so, am at least slightly but not totally surprised.
If you're not doing this, it seems worth considering.
As stated above, `solutions' only need to be rough.
And, of course, one could change K from 2 to something bigger.

This all said, there should still be some use of randomness in the search.

I hope that the above is clear.
I hope it's accurate.
I hope that it or a simplified or more general variant might be useful.

\begin{thebibliography}{}

% Note: I include the DOI as a comment on the same line after every \bibitem
%       entry. I do this in case I ever need to extract full bibstems for the
%       bibliography, which can be done using this curl command and the DOI:

%       curl -LH "Accept: application/x-bibtex" http://dx.doi.org/10.5555/12345678 

%       If no DOI is available then I list the ADS identifier.
\end{thebibliography}

\end{document}
